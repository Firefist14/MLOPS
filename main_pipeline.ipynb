{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9433dfc-e3c6-446e-9a51-475c76de27c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# storage_account_name = \"jaistorageaccount\"\n",
    "# container_name = \"sample\"\n",
    "# mount_point = \"/mnt/rawdata\"  # Choose a mount point name relevant to your data\n",
    "\n",
    "# # Fetch service principal credentials from Databricks secrets\n",
    "# application_id = dbutils.secrets.get(scope=\"my-secret-scope\", key=\"application-id\")\n",
    "# authentication_key = dbutils.secrets.get(scope=\"my-secret-scope\", key=\"application-secret\")\n",
    "# tenant_id = dbutils.secrets.get(scope=\"my-secret-scope\", key=\"tenant-id\")\n",
    "# print(application_id)\n",
    "# print(authentication_key)\n",
    "# print(tenant_id)\n",
    "# endpoint = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"\n",
    "\n",
    "# configs = {\n",
    "#   \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "#   \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "#   \"fs.azure.account.oauth2.client.id\": application_id,\n",
    "#   \"fs.azure.account.oauth2.client.secret\": authentication_key,\n",
    "#   \"fs.azure.account.oauth2.client.endpoint\": endpoint\n",
    "# }\n",
    "\n",
    "# # Source URI including container and storage account\n",
    "# source = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\"\n",
    "\n",
    "# # Mount only if not already mounted\n",
    "# if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "#     dbutils.fs.mount(\n",
    "#         source = source,\n",
    "#         mount_point = mount_point,\n",
    "#         extra_configs = configs\n",
    "#     )\n",
    "#     print(f\"Mount successful at {mount_point}\")\n",
    "# else:\n",
    "#     print(f\"{mount_point} is already mounted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9780286-cab8-4b0f-b450-db4515d2fa90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, hour\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EndToEndPipeline\").getOrCreate()\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df = df.withColumn(\"Issued_date\", col(\"Issued_date\").cast(\"timestamp\"))\n",
    "    df = df.withColumn(\"Issued_year\", year(\"Issued_date\")) \\\n",
    "           .withColumn(\"Issued_month\", month(\"Issued_date\")) \\\n",
    "           .withColumn(\"Issued_day\", dayofmonth(\"Issued_date\")) \\\n",
    "           .withColumn(\"Issued_hour\", hour(\"Issued_date\")) \\\n",
    "           .drop(\"Issued_date\")\n",
    "\n",
    "    drop_cols = ['Unit_ID', 'Violation_ID', 'Tract']\n",
    "    for c in drop_cols:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c)\n",
    "\n",
    "    # Handle categorical columns - encode all string columns\n",
    "    cat_cols = [c for c, t in df.dtypes if t == 'string']\n",
    "    for c in cat_cols:\n",
    "        indexer = StringIndexer(inputCol=c, outputCol=c+\"_index\", handleInvalid='keep')\n",
    "        df = indexer.fit(df).transform(df).drop(c).withColumnRenamed(c+\"_index\", c)\n",
    "\n",
    "    return df\n",
    "\n",
    "def replace_missing(df):\n",
    "    numeric_cols = [f.name for f in df.schema.fields if str(f.dataType) in ['IntegerType', 'DoubleType', 'LongType', 'FloatType']]\n",
    "    for col_name in numeric_cols:\n",
    "        median_val = df.approxQuantile(col_name, [0.5], 0.01)[0]\n",
    "        df = df.na.fill({col_name: median_val})\n",
    "    return df\n",
    "\n",
    "def select_features(df):\n",
    "    drop_cols = ['License_Plate_State', 'Tract', 'Unit_ID', 'Violation_ID']\n",
    "    for c in drop_cols:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(c)\n",
    "    return df\n",
    "\n",
    "def split_data(df, train_fraction=0.8):\n",
    "    return df.randomSplit([train_fraction, 1-train_fraction], seed=42)\n",
    "\n",
    "def train_and_register(train_df, test_df, model_output_path, registered_model_name):\n",
    "    train_pd = train_df.toPandas()\n",
    "    test_pd = test_df.toPandas()\n",
    "\n",
    "    target = \"PaymentIsOutstanding\"\n",
    "    le = LabelEncoder()\n",
    "    train_pd[target] = le.fit_transform(train_pd[target].astype(str))\n",
    "    test_pd[target] = le.transform(test_pd[target].astype(str))\n",
    "\n",
    "    X_train = train_pd.drop(columns=[target])\n",
    "    y_train = train_pd[target]\n",
    "    X_test = test_pd.drop(columns=[target])\n",
    "    y_test = test_pd[target]\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    os.makedirs(model_output_path, exist_ok=True)\n",
    "    model_file = os.path.join(model_output_path, \"model.joblib\")\n",
    "    joblib.dump(model, model_file)\n",
    "\n",
    "    # Evaluate and log metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    rec = recall_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\", registered_model_name=registered_model_name)\n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"recall\", rec)\n",
    "        mlflow.log_metric(\"precision\", prec)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    print(f\"Model training and registration successful! Model trained, registered as '{registered_model_name}', and metrics logged.\")\n",
    "\n",
    "def run_pipeline(raw_data_path, model_output_path, registered_model_name):\n",
    "    df = spark.read.option(\"header\", \"true\").csv(raw_data_path)\n",
    "    df = feature_engineering(df)\n",
    "    df = replace_missing(df)\n",
    "    df = select_features(df)\n",
    "    train_df, test_df = split_data(df)\n",
    "\n",
    "    train_and_register(train_df, test_df, model_output_path, registered_model_name)\n",
    "\n",
    "# Example run\n",
    "if __name__ == \"__main__\":\n",
    "    raw_path = \"/dbfs/mnt/rawdata/reduced_file.csv\"\n",
    "    model_path = \"/mnt/models\"\n",
    "    registered_name = \"my_spark_native\"\n",
    "    run_pipeline(raw_path, model_path, registered_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f45f644-530f-457c-97c9-11e65c8005a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import mlflow.pyfunc\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load model by name and version (use latest version by default)\n",
    "# model_name = \"my_spark_native_model\"\n",
    "# model_version = 1  # or omit to get latest version\n",
    "\n",
    "# model_uri = f\"models:/{model_name}/{model_version}\"  \n",
    "\n",
    "# # Load model as a PyFunc model (generic for most flavors)\n",
    "# model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# # Predict on new data - input as Pandas DataFrame or Spark DataFrame (converted to pandas)\n",
    "# data = pd.DataFrame({\n",
    "#     \"Issued_year\": [2023],\n",
    "#     \"Issued_month\": [5],\n",
    "#     \"Issued_day\": [3],\n",
    "#     \"Issued_hour\": [10],\n",
    "#     \"Community_Name\": [1],\n",
    "#     \"Sector\": [1],\n",
    "#     \"Side\": [1],\n",
    "#     \"Hardship_Index\": [50.5],\n",
    "#     \"Per_capita_income\": [15957.00],\n",
    "#     \"Percent_unemployed\": [5.0],\n",
    "#     \"Percent_without_diploma\": [10.0],\n",
    "#     \"Percent_households_below_poverty\": [12.0],\n",
    "#     \"Neighborhood\": [1],\n",
    "#     \"Ward\": [1],\n",
    "#     \"ZIP\": [12345],\n",
    "#     \"Police_District\": [1],\n",
    "#     \"Plate_Type\": [1],\n",
    "#     # Add other features your model requires\n",
    "# })\n",
    "\n",
    "# # If Spark DataFrame, convert to pandas\n",
    "# if hasattr(data, \"toPandas\"):\n",
    "#     data = data.toPandas()\n",
    "\n",
    "# predictions = model.predict(data)\n",
    "# print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d979ca-bf6d-490f-b817-02fd59bb31eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
